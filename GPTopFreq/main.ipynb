{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataloading import DataLoader, Dataset\n",
    "from model import GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(dataset_name, split):\n",
    "    save_path = f'model_pred_and_gt/{dataset_name}/run_{split}'\n",
    "    for i in ['test', 'valid']:\n",
    "        for j in ['gt', 'pred']:\n",
    "            name = f'{j}_{i}'\n",
    "            os.makedirs( os.path.join(save_path, name), exist_ok=True )\n",
    "\n",
    "def save_res(path,dataset_name, model: GP, loader: DataLoader, mode, split=0):\n",
    "    gt = loader.gen_gt()\n",
    "    pred = model.predict_last(loader)\n",
    "    make_dirs(dataset_name, split)\n",
    "    if mode == 'test':\n",
    "        np.savetxt(os.path.join(path, 'gt_test') + '/data.csv', gt, delimiter=',')\n",
    "        np.savetxt(os.path.join(path, 'pred_test', 'data.csv'), pred, delimiter=',')\n",
    "    if mode == 'valid':\n",
    "        # print(os.path.join(path, 'gt_valid')+ '/data.csv')\n",
    "        np.savetxt(os.path.join(path, 'gt_valid')+ '/data.csv', gt, delimiter=',')\n",
    "        np.savetxt(os.path.join(path, 'pred_valid', 'data.csv'), pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'synthea_preprocessed'\n",
    "for split in [0,1,2,3,4]:\n",
    "    # split = 0\n",
    "    dataset = Dataset(dataset_name, split)\n",
    "    train_loader, valid_loader, test_loader = dataset.gen_dataloaders()\n",
    "    # print(len(valid_loader))\n",
    "    model = GP(dataset.vocab_size)\n",
    "    model.fit(train_loader)\n",
    "    # make_dirs(dataset_name)\n",
    "    save_res(f'model_pred_and_gt/{dataset_name}/run_{split}', dataset_name, model, valid_loader, 'valid', split=split)\n",
    "    save_res(f'model_pred_and_gt/{dataset_name}/run_{split}', dataset_name, model, test_loader, 'test', split=split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was written with regard to old data format, so you should convert tambn-format to this one to launch this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_file(path, type_of_split, data, prefix, encoding):\n",
    "    tmp_path = os.path.join(path, prefix + type_of_split + '.pkl')\n",
    "    with open(tmp_path, 'rb') as file:\n",
    "        data[type_of_split] = pickle.load(file, encoding=encoding)[type_of_split]\n",
    "    return data\n",
    "\n",
    "def retrieve_dict(path, prefix, encoding):\n",
    "    tmp_path = os.path.join(path, prefix + 'dev.pkl')\n",
    "    # print(encoding)\n",
    "    with open(tmp_path, 'rb') as file:\n",
    "        data = pickle.load(file, encoding=encoding)\n",
    "    unpickle_file(path, 'train', data, prefix, encoding=encoding)\n",
    "    unpickle_file(path, 'test', data, prefix, encoding=encoding)\n",
    "    return data\n",
    "\n",
    "def change_busket_enc_format(one_hot_encoded: np.ndarray) -> np.ndarray:\n",
    "    '''if the busket is empty'''\n",
    "    return np.nonzero(one_hot_encoded)[0].tolist()\n",
    "\n",
    "def retrieve_busket_seq(TCMBN_data_dict_record: list) -> list:\n",
    "    '''\n",
    "    TCMBN_data_dict_record is expected to be list with info corresponding to one user id\n",
    "    '''\n",
    "    busket_seq = []\n",
    "\n",
    "    for record in TCMBN_data_dict_record:\n",
    "        one_hot = record['type_event']\n",
    "        if ~(np.all(one_hot == 0)):\n",
    "            busket_seq.append(change_busket_enc_format(one_hot))\n",
    "        # busket_seq.append(change_busket_enc_format(one_hot))\n",
    "    \n",
    "    return busket_seq\n",
    "\n",
    "# def merge_splits_from_TCMBN(TCMBN_unpickled_data: dict) -> None:\n",
    "#     TCMBN_merged_arr = []\n",
    "\n",
    "#     for type_of_split in ['train', 'test', 'dev']:\n",
    "#         TCMBN_merged_arr.extend(TCMBN_unpickled_data[type_of_split])\n",
    "\n",
    "#     return TCMBN_merged_arr\n",
    "\n",
    "def create_DNNTSP_dict(TCMBN_arr: list, last_id = 0) -> dict:\n",
    "    '''\n",
    "    TCMBN_merged_arr is expected to be a list with data\n",
    "    TCMBN_merged_arr[i] returns data regarding i-th id  \n",
    "    '''\n",
    "\n",
    "    DNNTSP_dict = {}\n",
    "\n",
    "    for id, record in enumerate(TCMBN_arr):\n",
    "        busket_seq = retrieve_busket_seq(record)\n",
    "        if busket_seq:  # Only add non-empty sequences\n",
    "            DNNTSP_dict[str(last_id + id)] = busket_seq\n",
    "\n",
    "    return DNNTSP_dict\n",
    "\n",
    "def train_test_valid_split(DNNTSP_dict_: list, ratio, seed) -> dict:\n",
    "    '''\n",
    "    ration is expected as [train, test, valid], for e.g. [0.7, 0.15, 0.15]\n",
    "    '''\n",
    "\n",
    "    DNNTSP_dict = DNNTSP_dict_['train'] | DNNTSP_dict_['test'] | DNNTSP_dict_['validate'] \n",
    "\n",
    "    type_of_split = ['train', 'test', 'validate']\n",
    "\n",
    "    # ids_train = list(DNNTSP_dict['train'].keys())\n",
    "    # ids_test = list(DNNTSP_dict['test'].keys())\n",
    "    # ids_val = list(DNNTSP_dict['validate'].keys())\n",
    "\n",
    "    ids = list(DNNTSP_dict.keys())\n",
    "\n",
    "    random.seed(seed)\n",
    "    # random.shuffle(ids_train)\n",
    "    # random.shuffle(ids_test)\n",
    "    # random.shuffle(ids_val)\n",
    "\n",
    "    # ids = ids_train + ids_test + ids_val\n",
    "\n",
    "    random.shuffle(ids)\n",
    "\n",
    "    num_of_ids = len(ids)\n",
    "\n",
    "    # print(num_of_ids)\n",
    "\n",
    "    \n",
    "\n",
    "    split_len = {}\n",
    "    for ind, r in enumerate(ratio):\n",
    "        split_len[type_of_split[ind]] = int(r*num_of_ids)\n",
    "\n",
    "    # print(ratio)\n",
    "    # print(split_len)\n",
    "\n",
    "    # print(len(ids[:split_len['train']]))\n",
    "    # print(len(ids[split_len['train']: split_len['train'] + split_len['test']]))\n",
    "    \n",
    "    train_dict = {id : DNNTSP_dict[id] for id in ids[:split_len['train']]}\n",
    "    test_dict = {id : DNNTSP_dict[id] for id in ids[split_len['train']: split_len['train'] + split_len['test']]}\n",
    "    valid_dict = {id : DNNTSP_dict[id] for id in ids[split_len['train'] + split_len['test']:]}\n",
    "\n",
    "    DNNTSP_dict = {\n",
    "        'train': train_dict,\n",
    "        'test': test_dict,\n",
    "        'validate': valid_dict\n",
    "    }\n",
    "\n",
    "    # print(len(valid_dict))\n",
    "\n",
    "    return DNNTSP_dict\n",
    "\n",
    "def save_TCMBN_to_DNNTSP_format(dataset_name: str, path_to_pickled_files: str, ratio=[0.6,0.2,0.2], seed=42, prefix='', encoding='ASCII') -> None:\n",
    "    '''\n",
    "    given the path to folder with pickles saves prepared file in the noted path\n",
    "    also rearranges train/test split to ratio from TCMBN\n",
    "    '''\n",
    "\n",
    "    splits = {'train' : 'train', 'test' : 'test', 'dev' : 'validate'}\n",
    "    ids = {'train' : 0, 'test' : 10000, 'dev' : 20000}\n",
    "    \n",
    "\n",
    "    TCMBN_unpickled_data = retrieve_dict(path_to_pickled_files, prefix, encoding=encoding)\n",
    "    DNNTSP_dict = {}\n",
    "\n",
    "    for key, value in splits.items(): \n",
    "        DNNTSP_dict[value] = create_DNNTSP_dict(TCMBN_unpickled_data[key], ids[key])\n",
    "\n",
    "    \n",
    "    # print(DNNTSP_dict.keys())\n",
    "    # print(DNNTSP_dict['train'].keys())\n",
    "    DNNTSP_dict = train_test_valid_split(DNNTSP_dict, [0.6, 0.2, 0.2], seed=seed)\n",
    "\n",
    "    new_path = os.path.join('data', dataset_name, f'split_{seed}')\n",
    "    os.makedirs(new_path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(new_path, dataset_name) + '.json', 'w') as file:\n",
    "        json.dump(DNNTSP_dict, file)\n",
    "\n",
    "    return DNNTSP_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'instacart_preprocessed'\n",
    "path_to_pickled_files = f'../tcmbn_data/{dataset_name}/split_1'\n",
    "for seed in [0]:\n",
    "    new_dict = save_TCMBN_to_DNNTSP_format(dataset_name, path_to_pickled_files, prefix='', encoding='latin-1', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIPT_DL_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
